
<!-- saved from url=(0045)http://web.mst.edu/~gosavia/mrrl_website.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><title>Matlab Repository for Reinforcement Learning</title>
<style type="text/css"></style></head><body bgcolor="#CCFFCC" text="#000000" link="#0000ff" vlink="#ff0000">
<p></p><center> <font size="+2"><font color="#9933FF"><u>
MATLAB Repository for Reinforcement Learning</u></font></font></center><p></p>
<br>

<hr size="30" noshade="">

<p> Funded by the <a href="http://www.nsf.gov/"> National Science Foundation </a> via grant ECS: 0841055.
<img src="./Matlab Repository for Reinforcement Learning_files/nsf_logo.gif" width="128" height="128" border="0" align="top">
 
</p><p> The purpose of this web-site is to provide MATLAB codes for Reinforcement Learning (RL), which is also called Adaptive 
or Approximate Dynamic Programming (ADP) or Neuro-Dynamic Programming (NDP). 

</p><p> 


</p><p> <i>
This website has been created for the purpose of making RL programming 
accesible in the engineering community which widely uses MATLAB.  Please feel free to use these codes in your research.
I will appreciate it if you send me an email acknowledging 
their use in your research. </i> 
</p><p>

The first set of codes that we provide use a 2-state Markov chain as the test bed.  While this is a simple test-bed, it is useful 
to test a new algorithm.  Also, these codes are meant to give you an idea of how to incorporate a Q-learning algorithm within a 
discrete-event simulator  of  your own. Please  click <a href="http://web.mst.edu/~gosavia/codes/wsccodes.html"> here </a> 
to access these codes.

</p><p> Codes are provided for Q-learning, R-SMART and also for value iteration (Q-factor versions).
</p><p> For a tutorial on RL, please click <a href="http://web.mst.edu/~gosavia/tutorial.pdf"> here </a>.

</p><p> Matlab Codes for the following paper on semi-variance penalized MDPs and SMDPs (survival probabilities): 
</p><p>

1. A. Gosavi.<a href="http://web.mst.edu/~gosavia/semi_variance.pdf"> Target-sensitive control of Markov and semi-Markov processes, </a> <i> International Journal of Control, Automation, and 
Systems, </i>, 9(5):1-11, 2011. 
</p><p>
can be found <a href="http://web.mst.edu/~gosavia/semi-variance"> here </a>.


</p><p>  Other papers that were partially funded from this project include:

</p><p>

2. Abhijit Gosavi. <a href="http://web.mst.edu/~gosavia/joc.pdf">"Reinforcement Learning: A Tutorial Survey and Recent Advances." (pdf file) </a> <i> INFORMS Journal on Computing,</i>
21(2):178-192, 2009.
</p><p>



3. Abhijit Gosavi, Susan L. Murray, Jiaqiao Hu, and Shuva Ghosh.  .<a href="http://web.mst.edu/~gosavia/smac_paper.pdf"> Model-building Adaptive Critics for semi-Markov Control. </a>
<i> Journal of Artificial Intelligence and Soft Computing Research</i>, 2(1), 2012.  


</p><p> 
4. A. Gosavi, S.L. Murray, V.M. Tirumalasetty and S. Shewade.  <a href="http://web.mst.edu/~gosavia/shewade.pdf">  
A Budget-Sensitive Approach to Scheduling Maintenance in a Total Productive Maintenance (TPM) Program </a>, <i>Engineering Management Journal </i>, 23(3): 46-56, 2011.
</p><p>

5. K. Kulkarni, A. Gosavi, S. L. Murray and K. Grantham <a href="http://web.mst.edu/~gosavia/kulkarni.pdf"> 
Semi-Markov Adaptive Critic Heuristics with Application to Airline
Revenue Management </a> <i> Journal of Control Theory and Applications </i>(special issue on Approximate Dynamic Programming), 9(3): 421-430, 2011. 
</p><p>





</p><p> We plan to put up numerous other MATLAB codes for RL on this website!


</p><p>


<a href="http://web.mst.edu/~gosavia/index.html"> <b> Back to Abhijit Gosavi's homepage.</b> </a>




</p></body></html>