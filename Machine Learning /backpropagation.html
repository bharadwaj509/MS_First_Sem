<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0115)https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>backpropagation</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2014-10-28"><meta name="DC.source" content="backpropagation.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style><style type="text/css"></style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#2">Description of the function</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#3">Is problem well defined?</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#4">Network initialization</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#5">Activation: activation matrix a{i} for layer i</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#6">Net matrix</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#7">Define:</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#8">loop while epochs less than bound on epochs &amp;&amp; error &gt; errorbound</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#10">Calculate sum squared error of all samples</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#11">BACKPROPAGATION PHASE:</a></li><li><a href="https://canopy.uc.edu/bbcswebdav/pid-14184421-dt-content-rid-38653596_1/courses/15FS_CS6037001/backpropagation.html#12">Update the prev_w, weight matrices, epoch count and error</a></li></ul></div><pre class="codeinput"><span class="keyword">function</span> y = backpropagation(X,Target, L,eta,alpha, errorbound, epochsbound)
</pre><h2>Description of the function<a name="2"></a></h2><p>INVOKE: y = backpropagation(X,Target, L,eta,alpha, errorbound, epochsbound)</p><p>Implements backprop where backpropagation training is done per epoch;  three layers feed forward  neural network.</p><p>It returns y the network which has the structure as follows:     S: a vector of number of nodes at each layer     W - a cell array specifiying the final weight matrices computed     E - the epochs required for training     MSE - the mean squared error at termination</p><p>Input:    Layers - a vector of integers specifying the number of nodes at each    layer, i.e for all i,    Layers(i) = number of nodes at layer i;         there must be (at least/exactly) three layers and the input layer    Layers(1) equals the dimension of an input vector;    Layers(end) equals the dimension of each vector in Target    Layers(2:end-1) are hidden layers; in general we will have ONLY ONE    HIDDEN LAYER</p><pre class="codeinput"><span class="comment">%     eta - training rate for network learning [0.1 - 0.9]</span>
<span class="comment">%     alpha - momentum for the weight update rule [0.0 - 0.9]</span>
<span class="comment">%     errorbound - the mse at which to terminate computation</span>
<span class="comment">%</span>
<span class="comment">%     The training samples, X, is a P-by-N matrix, where each X(p,:) is</span>
<span class="comment">%     a training vector of dimension N.</span>
<span class="comment">%</span>
<span class="comment">%    Target - the Target outputs, a P-by-M matrix where each Target[p]</span>
<span class="comment">%      is the Target output for the corresponding input Input[p]</span>
<span class="comment">%      Target(p) is a vectore of dimension M</span>
<span class="comment">%   epochsbound: bound on the number of epochs</span>
<span class="comment">%   W is a cell array;  weight matrices obtained from</span>
<span class="comment">%   minimizing the mean squared error between the Target and network output</span>
<span class="comment">%   based on the training examples X and the errorbound</span>
<span class="comment">%</span>
<span class="comment">%   squashing functions:</span>
<span class="comment">%   Sigmoid: sigma(x) = 1/(1+ e^(-x))</span>
<span class="comment">%   Hyperbolic tangent:</span>
<span class="comment">%   ht(x) = 2/(1+e^(-x)) - 1  (for bipolar data)</span>
<span class="comment">%</span>
<span class="comment">%</span>
</pre><h2>Is problem well defined?<a name="3"></a></h2><p>Determine sizes of X and Target</p><pre class="codeinput">[P,N] = size(X);
[Pt,M] = size(Target);

<span class="comment">% P and Pt must be equal : each input vector has a corresponding Target output</span>
<span class="keyword">if</span> P ~= Pt
    error(<span class="string">'invalid backpropagation'</span>, <span class="keyword">...</span>
          <span class="string">'inputs != targets'</span>);
<span class="keyword">end</span>

<span class="comment">% Network has three layers: input - hidden - output</span>
<span class="keyword">if</span> length(L) &lt; 3
    error(<span class="string">'backprop:invalidNetworkStructure'</span>,<span class="string">'The network must have at least 3 layers'</span>);
<span class="keyword">else</span>
    <span class="keyword">if</span> N ~= L(1) || M ~= L(end)
        error(<span class="string">'backpropagation:invalid Layer Size'</span>, e);
    <span class="keyword">elseif</span> M ~= L(end)
        error(<span class="string">'backprop:invalidLayerSize'</span>, e);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">Error using backpropagation (line 49)
Not enough input arguments.
</pre><h2>Network initialization<a name="4"></a></h2><pre class="codeinput">numlayers = length(L);

<span class="comment">% Initialize weights: random values  U[-1 1],</span>
<span class="comment">% Weight matrix between each layer of nodes.</span>
<span class="comment">% The input layer and hidden layer have a bias node: weight is 0 corresponding to input 1</span>
<span class="comment">% There is a link from each node in layer i to the bias node in layer j</span>
<span class="comment">% (the last row of each matrix): it is more efficient than using the 1st</span>
<span class="comment">% row</span>
<span class="comment">%  The  weights of all links to bias nodes are irrelevant and are defined as 0</span>

w = cell(numlayers-1,1); <span class="comment">% a weight matrix for each layer</span>
<span class="keyword">for</span> i=1:numlayers-2
    [L(i+1) L(i)+1]
    1 - 2.*rand(L(i+1),L(i)+1) <span class="comment">% matrix for each pair of layers</span>
    w{i} = [1 - 2.*rand(L(i+1),L(i)+1) ; zeros(1,L(i)+1)]
<span class="keyword">end</span>
w{end} = 1 - 2.*rand(L(end),L(end-1)+1);

<span class="comment">% initialize stopping conditions</span>
err = Inf;  <span class="comment">% assuming the intial weight matrices are bad</span>
epochs = 0;
</pre><h2>Activation: activation matrix a{i} for layer i<a name="5"></a></h2><p>a{1} = the network input and a{end} = network output a{i} is a  P x K_i (P:no. of data points,K_i:nodes at layer i) matrix such that a{i}(j): activation vector for the jth input of layer i; a{i}(j,k) is the activation(output) of the kth node in layer i for the jth input</p><pre class="codeinput">a = cell(numlayers,1);  <span class="comment">% one activation matrix for each layer</span>
a{1} = [X ones(P,1)]; <span class="comment">% a{1} is the input + '1' for the bias node activation</span>
                      <span class="comment">% a{1} remains the same throught the computation</span>
<span class="keyword">for</span> i=2:numlayers-1
    a{i} = ones(P,L(i)+1); <span class="comment">% inner layers include a bias node (P-by-Nodes+1)</span>
<span class="keyword">end</span>
a{end} = ones(P,L(end));   <span class="comment">% no bias node at output layer</span>
</pre><h2>Net matrix<a name="6"></a></h2><p>Net Matrix: net{i} for layer i (not including the input layer): P x K matrix  net{i} is computed as dot product:  net{i}= sum(w(i,j) * a(j)) for j = i-1  net{i}(j)  =  net vector at layer i for the jth data point  net{i}(j,k) =  net input at node k of the ith layer for the jth sample</p><pre class="codeinput">net = cell(numlayers-1,1);
<span class="keyword">for</span> i=1:numlayers-2;
    net{i} = ones(P,L(i+1)+1); <span class="comment">% extend with the bias node</span>
<span class="keyword">end</span>
net{end} = ones(P,L(end));
</pre><h2>Define:<a name="7"></a></h2><p>prev_deltaw : delta weight P x K_i matrices at step (t-1) sum_deltaw: sum of the delta weights for each presentation of the input prev_deltaw{i}: delta weight matrix for all samples at step (t-1); sum_deltaw{i} : the sum of the weight matrix at layer i for all datapoints</p><pre class="codeinput">prev_deltaw = cell(numlayers-1,1);
sum_deltaw = cell(numlayers-1,1);
<span class="keyword">for</span> i=1:numlayers-1
    prev_deltaw{i} = zeros(size(w{i})); <span class="comment">% initialize prev_deltaw to 0</span>
    sum_deltaw{i} = zeros(size(w{i}));
<span class="keyword">end</span>
</pre><h2>loop while epochs less than bound on epochs &amp;&amp; error &gt; errorbound<a name="8"></a></h2><pre class="codeinput"><span class="keyword">while</span> err &gt; errorbound &amp;&amp; epochs &lt; epochsbound
</pre><pre class="codeinput">    <span class="comment">% FEEDFORWARD: propagate input through each layer for all data points</span>

    <span class="keyword">for</span> i=1:numlayers-1
        net{i} = a{i} * w{i}'; <span class="comment">% compute inputs to current layer</span>

        <span class="comment">% compute activation(output of current layer, for all layers</span>
        <span class="comment">% exclusive the output, the last node is the bias node and</span>
        <span class="comment">% its activation is 1</span>
        <span class="keyword">if</span> i &lt; numlayers-1 <span class="comment">% inner layers</span>
            <span class="comment">%a{i+1} = [2./(1+exp(-net{i}(:,1:end-1)))-1 ones(P,1)]; %tanh</span>
            a{i+1} = [1./(1+exp(-net{i}(:,1:end-1))) ones(P,1)];  <span class="comment">% sigmoid</span>
        <span class="keyword">else</span>             <span class="comment">% output layers</span>
            a{i+1} = 1./(1+exp(-net{i}));
        <span class="keyword">end</span>
    <span class="keyword">end</span>
</pre><h2>Calculate sum squared error of all samples<a name="10"></a></h2><pre class="codeinput">    err = (Target-a{end});       <span class="comment">% save this for later</span>
    sumsquarederror = sum(sum(err.^2)); <span class="comment">% sum of the error for all samples, and all nodes</span>
</pre><h2>BACKPROPAGATION PHASE:<a name="11"></a></h2><p>For the sigmoid function this is sigma'(O)= O(1-O); For the tanh this tanh'(O) = (1+O)*(1-O)</p><p>Start with the output nodes: 	Calculate the sum of the weight matrices for all samples:   eta * O(1-O)*(Target - O)*x</p><p>Backpropagate the error such that the modified error for this layer is: O(1-O)*(Activation) * ModifiedError * weight matrix</p><pre class="codeinput">    delta = err .* a{end}.* (1 - a{end});

    <span class="keyword">for</span> i=numlayers-1:-1:1
        sum_deltaw{i} = eta * delta' * a{i};
        <span class="keyword">if</span> i &gt; 1
            delta = a{i} .* (1-a{i}) .* (delta*w{i});
        <span class="keyword">end</span>
    <span class="keyword">end</span>
</pre><h2>Update the prev_w, weight matrices, epoch count and error<a name="12"></a></h2><pre class="codeinput">    <span class="keyword">for</span> i=1:numlayers-1
        <span class="comment">% we have the sum of the delta weights, divide through by the</span>
        <span class="comment">% number of samples and add momentum * delta weight at (t-1)</span>
        <span class="comment">% finally, update the weight matrices</span>
        prev_deltaw{i} = (sum_deltaw{i} ./ P) + (alpha * prev_deltaw{i});
        w{i} = w{i} + prev_deltaw{i};
    <span class="keyword">end</span>
    epochs = epochs + 1;
    err = sumsquarederror/(P*M); <span class="comment">% = 1/P * 1/M * summed squared error</span>
</pre><pre class="codeinput"><span class="keyword">end</span>

<span class="comment">% return the trained network</span>
y.S = L;
y.W = w;
y.E = epochs;
y.MSE = err;
</pre><p class="footer"><br>
      Published with MATLABÂ® 7.13<br></p></div><!--
##### SOURCE BEGIN #####
function y = backpropagation(X,Target, L,eta,alpha, errorbound, epochsbound)
%% Description of the function
% INVOKE: 
% y = backpropagation(X,Target, L,eta,alpha, errorbound, epochsbound)
% 
% Implements backprop where backpropagation training is done per epoch;
%  three layers feed forward  neural network.
%   
% It returns y the network which has the structure as follows:
%     S: a vector of number of nodes at each layer
%     W - a cell array specifiying the final weight matrices computed
%     E - the epochs required for training
%     MSE - the mean squared error at termination
%   
% Input:
%    Layers - a vector of integers specifying the number of nodes at each
%    layer, i.e for all i, 
%    Layers(i) = number of nodes at layer i;
%         there must be (at least/exactly) three layers and the input layer 
%    Layers(1) equals the dimension of an input vector;
%    Layers(end) equals the dimension of each vector in Target
%    Layers(2:end-1) are hidden layers; in general we will have ONLY ONE
%    HIDDEN LAYER

%     eta - training rate for network learning [0.1 - 0.9]
%     alpha - momentum for the weight update rule [0.0 - 0.9]
%     errorbound - the mse at which to terminate computation
%     
%     The training samples, X, is a P-by-N matrix, where each X(p,:) is
%     a training vector of dimension N.
%
%    Target - the Target outputs, a P-by-M matrix where each Target[p]
%      is the Target output for the corresponding input Input[p]
%      Target(p) is a vectore of dimension M
%   epochsbound: bound on the number of epochs
%   W is a cell array;  weight matrices obtained from  
%   minimizing the mean squared error between the Target and network output 
%   based on the training examples X and the errorbound 
%
%   squashing functions:
%   Sigmoid: sigma(x) = 1/(1+ e^(-x))
%   Hyperbolic tangent:
%   ht(x) = 2/(1+e^(-x)) - 1  (for bipolar data)
%   
%  

%% Is problem well defined?
% Determine sizes of X and Target
[P,N] = size(X);
[Pt,M] = size(Target);

% P and Pt must be equal : each input vector has a corresponding Target output
if P ~= Pt
    error('invalid backpropagation', ...
          'inputs != targets');
end

% Network has three layers: input - hidden - output
if length(L) < 3 
    error('backprop:invalidNetworkStructure','The network must have at least 3 layers');
else
    if N ~= L(1) || M ~= L(end)
        error('backpropagation:invalid Layer Size', e);
    elseif M ~= L(end)
        error('backprop:invalidLayerSize', e);    
    end
end

%%  Network initialization 

numlayers = length(L); 

% Initialize weights: random values  U[-1 1], 
% Weight matrix between each layer of nodes. 
% The input layer and hidden layer have a bias node: weight is 0 corresponding to input 1
% There is a link from each node in layer i to the bias node in layer j 
% (the last row of each matrix): it is more efficient than using the 1st
% row
%  The  weights of all links to bias nodes are irrelevant and are defined as 0

w = cell(numlayers-1,1); % a weight matrix for each layer
for i=1:numlayers-2   
    [L(i+1) L(i)+1]
    1 - 2.*rand(L(i+1),L(i)+1) % matrix for each pair of layers
    w{i} = [1 - 2.*rand(L(i+1),L(i)+1) ; zeros(1,L(i)+1)]
end
w{end} = 1 - 2.*rand(L(end),L(end-1)+1);

% initialize stopping conditions
err = Inf;  % assuming the intial weight matrices are bad
epochs = 0;


%% Activation: activation matrix a{i} for layer i 
% a{1} = the network input and 
% a{end} = network output
% a{i} is a  P x K_i (P:no. of data points,K_i:nodes at layer i) matrix such that 
% a{i}(j): activation vector for the jth input of layer i; 
% a{i}(j,k) is the activation(output) of the kth node in layer i for the jth 
% input

a = cell(numlayers,1);  % one activation matrix for each layer
a{1} = [X ones(P,1)]; % a{1} is the input + '1' for the bias node activation
                      % a{1} remains the same throught the computation
for i=2:numlayers-1
    a{i} = ones(P,L(i)+1); % inner layers include a bias node (P-by-Nodes+1) 
end
a{end} = ones(P,L(end));   % no bias node at output layer

%% Net matrix
% Net Matrix: net{i} for layer i (not including the input layer): P x K
% matrix
%  net{i} is computed as dot product:
%  net{i}= sum(w(i,j) * a(j)) for j = i-1
%  net{i}(j)  =  net vector at layer i for the jth data point
%  net{i}(j,k) =  net input at node k of the ith layer for the jth sample

net = cell(numlayers-1,1);
for i=1:numlayers-2;
    net{i} = ones(P,L(i+1)+1); % extend with the bias node 
end
net{end} = ones(P,L(end));

%% Define:
% prev_deltaw : delta weight P x K_i matrices at step (t-1) 
% sum_deltaw: sum of the delta weights for each presentation of the input
% prev_deltaw{i}: delta weight matrix for all samples at step (t-1);
% sum_deltaw{i} : the sum of the weight matrix at layer i for all
% datapoints

prev_deltaw = cell(numlayers-1,1);
sum_deltaw = cell(numlayers-1,1);
for i=1:numlayers-1
    prev_deltaw{i} = zeros(size(w{i})); % initialize prev_deltaw to 0
    sum_deltaw{i} = zeros(size(w{i}));
end    

%% loop while epochs less than bound on epochs && error > errorbound

while err > errorbound && epochs < epochsbound
    % FEEDFORWARD: propagate input through each layer for all data points
    
    for i=1:numlayers-1
        net{i} = a{i} * w{i}'; % compute inputs to current layer
        
        % compute activation(output of current layer, for all layers
        % exclusive the output, the last node is the bias node and
        % its activation is 1
        if i < numlayers-1 % inner layers
            %a{i+1} = [2./(1+exp(-net{i}(:,1:end-1)))-1 ones(P,1)]; %tanh
            a{i+1} = [1./(1+exp(-net{i}(:,1:end-1))) ones(P,1)];  % sigmoid
        else             % output layers
            a{i+1} = 1./(1+exp(-net{i})); 
        end
    end
    
    %% Calculate sum squared error of all samples
    err = (Target-a{end});       % save this for later
    sumsquarederror = sum(sum(err.^2)); % sum of the error for all samples, and all nodes
    
    %% BACKPROPAGATION PHASE: 
    % For the sigmoid function this is sigma'(O)= O(1-O);
    % For the tanh this tanh'(O) = (1+O)*(1-O)
    % 
    % Start with the output nodes:
    % 	Calculate the sum of the weight matrices for all samples: 
    %   eta * O(1-O)*(Target - O)*x
    % 
    % Backpropagate the error such that the modified error for this
    % layer is: O(1-O)*(Activation) * ModifiedError * weight matrix
   
    delta = err .* a{end}.* (1 - a{end});

    for i=numlayers-1:-1:1
        sum_deltaw{i} = eta * delta' * a{i};
        if i > 1
            delta = a{i} .* (1-a{i}) .* (delta*w{i});
        end
    end
    
 %% Update the prev_w, weight matrices, epoch count and error
    for i=1:numlayers-1
        % we have the sum of the delta weights, divide through by the 
        % number of samples and add momentum * delta weight at (t-1)
        % finally, update the weight matrices
        prev_deltaw{i} = (sum_deltaw{i} ./ P) + (alpha * prev_deltaw{i});
        w{i} = w{i} + prev_deltaw{i};
    end   
    epochs = epochs + 1;
    err = sumsquarederror/(P*M); % = 1/P * 1/M * summed squared error
end

% return the trained network
y.S = L;
y.W = w;
y.E = epochs;
y.MSE = err;
##### SOURCE END #####
--></body></html>